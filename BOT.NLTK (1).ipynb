{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOgQ9HkawU89YMkO1gOhKRu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"FzGdaG-rU4b5"},"outputs":[],"source":["import random\n","import json\n","import pickle\n","import numpy as np\n","\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation, Dropout\n","from keras.optimizers import Adam, SGD\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","with open(\"intents.json\", encoding=\"utf-8\") as f:\n","    intents = json.load(f)\n","nltk.download(\"punkt\")\n","nltk.download(\"wordnet\")\n","nltk.download(\"omw-1.4\")\n","\n","words = []\n","classes = []\n","documents = []\n","ignore_letters = [\"!\",\"?\",\"¡\",\"¿\",\".\",\",\",\"/\"]\n","\n","#Clasifica los patrones y las categorías\n","for intent in intents[\"intents\"]:\n","    for pattern in intent[\"patrones\"]:\n","        word_list = nltk.word_tokenize(pattern)\n","        words.extend(word_list)\n","        documents.append((word_list, intent[\"tag\"]))\n","        if intent[\"tag\"] not in classes:\n","            classes.append(intent[\"tag\"])\n","\n","words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]\n","words = sorted(set(words))\n","\n","pickle.dump(words, open(\"words.pkl\", \"wb\"))\n","pickle.dump(classes, open(\"classes.pkl\", \"wb\"))\n","\n","\n","#Pasa la información a unos y ceros según las palabras presentes en cada categoría para hacer el entrenamiento\n","training = []\n","outoput_empty = [0]*len(classes)\n","for document in documents:\n","    bag = []\n","    word_patterns = document[0]\n","    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n","    for word in words:\n","        bag.append(1) if word_patterns else bag.append(0)\n","    output_row = list(outoput_empty)\n","    output_row[classes.index(document[1])] = 1\n","    training.append([bag, output_row])\n","\n","random.shuffle(training)\n","training = np.array(training, dtype=object)\n","print(training)\n","\n","#Reparte los datos para pasarlos a la red\n","train_x = list(training[:,0])\n","train_y = list(training[:,1])\n","\n","#Red neuronal\n","model = Sequential()\n","model.add(Dense(512, input_shape=(len(train_x[0]),), activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(256, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(len(train_y[0]), activation='softmax'))\n","\n","\n","#Optimizador y lo compilamos\n","sgd = Adam(learning_rate=0.001)\n","model.compile(loss='categorical_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n","\n","#Entrenamiento el modelo y lo guardamos\n","train_process = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=32, verbose=1)\n","model.save(\"chatbot_model.h5\", train_process)\n","\n","print(\"Model created\")\n"]},{"cell_type":"code","source":["import random\n","import json\n","import pickle\n","import numpy as np\n","\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","\n","from keras.models import load_model\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","#Importamos los archivos generados en el código anterior\n","with open('intents.json', encoding='utf-8') as file:\n","    intents = json.load(file)\n","words = pickle.load(open('words.pkl', 'rb'))\n","classes = pickle.load(open('classes.pkl', 'rb'))\n","model = load_model('chatbot_model.h5')\n","\n","\n","#Pasamos las palabras de oración a su forma raíz\n","def clean_up_sentence(sentence):\n","    sentence_words = nltk.word_tokenize(sentence)\n","    sentence_words = [lemmatizer.lemmatize(word) for word in sentence_words]\n","    return sentence_words\n","\n","#Convertimos la información a unos y ceros según si están presentes en los patrones\n","def bag_of_words(sentence):\n","    sentence_words = clean_up_sentence(sentence)\n","    bag = [0]*len(words)\n","    for w in sentence_words:\n","        for i, word in enumerate(words):\n","            if word == w:\n","                bag[i]=1\n","    print(bag)\n","    return np.array(bag)\n","\n","#Predecimos la categoría a la que pertenece la oración\n","def predict_class(sentence):\n","    bow = bag_of_words(sentence)\n","    res = model.predict(np.array([bow]))[0]\n","    max_index = np.where(res ==np.max(res))[0][0]\n","    category = classes[max_index]\n","    return category\n","\n","#Obtenemos una respuesta aleatoria\n","def get_response(tag, intents_json):\n","    list_of_intents = intents_json['intents']\n","    result = \"\"\n","    for i in list_of_intents:\n","        if i[\"tag\"]==tag:\n","            result = random.choice(i['respuestas'])\n","            break\n","    return result\n","\n","#Ejecutamos el chat en bucle\n","while True:\n","    message=input(\"\")\n","    ints = predict_class(message)\n","    res = get_response(ints, intents)\n","    print(res)"],"metadata":{"id":"E92xF6-pWAXQ"},"execution_count":null,"outputs":[]}]}